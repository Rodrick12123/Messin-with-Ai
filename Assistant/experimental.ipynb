{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import requests\n",
    "from huggingface_hub import InferenceClient\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Get token from .env file\n",
    "hf_token = os.getenv('HUGGING_FACE_TOKEN')\n",
    "\n",
    "if not hf_token:\n",
    "    raise ValueError(\"HUGGING_FACE_TOKEN not found in .env file\")\n",
    "\n",
    "print(\"Token loaded successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "client = InferenceClient(api_key=hf_token)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PersonalAssistant:\n",
    "    def __init__(self, client, role=\"You are a personal assistant that responds to a users input\"):\n",
    "        self.client = client\n",
    "        self.system_role = role\n",
    "        self.commands = [\"generate books\"]  # Add more commands as needed\n",
    "\n",
    "    def execute_command(self,command):\n",
    "        print(f\"Executing command: {command}...\")\n",
    "        # Add logic for executing the specific command here\n",
    "\n",
    "    def interact_with_model(self):\n",
    "      \"\"\"\n",
    "      Continuously ask the user for input and interact with the model.\n",
    "      \"\"\"\n",
    "      while True:\n",
    "          # Get user input for the next question or query\n",
    "          user_input = input(\"Ask a question (or type 'exit' to stop): \").strip()\n",
    "\n",
    "          if user_input.lower() == 'exit': \n",
    "              print(self.speak(user_input))\n",
    "              print(\"Exiting the program.\")\n",
    "              break\n",
    "\n",
    "          #Check if user input is directly a command\n",
    "          if user_input.lower() in (cmd.lower() for cmd in self.commands):\n",
    "              self.execute_command(user_input.lower())\n",
    "              continue\n",
    "\n",
    "          # Pass the input to the speak function and get the model's response\n",
    "          model_response = self.speak(user_input=user_input)\n",
    "\n",
    "          # Check if the model's response matches a command\n",
    "          if model_response.lower() in (cmd.lower() for cmd in self.commands):\n",
    "              self.execute_command(model_response.lower())\n",
    "              \n",
    "          else:\n",
    "              print(f\"Model response: {model_response}\")\n",
    "\n",
    "\n",
    "    def speak(self, user_input=\"None\"):\n",
    "        \"\"\"\n",
    "        Generate a response based on the user input and predefined commands.\n",
    "        \"\"\"\n",
    "        # Prepare the model input prompt based on user input\n",
    "        if user_input.lower() == \"none\":\n",
    "            prompt = \"User has not said anything yet. Respond in a detailed and informative manner.\"\n",
    "        elif user_input.lower() == 'exit':\n",
    "            prompt = \"User wishes to exit the program. Wish them farewell. Your response should only include the farewell message.\"\n",
    "        else:\n",
    "            prompt = (\n",
    "                f\"Case 1: If the user input: {user_input} matches or looks like one of these commands: {self.commands}, \"\n",
    "                \"respond only with the command name. \"\n",
    "                f\"Case 2: If the user input: {user_input} does not match any command in the list {self.commands}, \"\n",
    "                \"acknowledge the user input and respond that you have a list of commands, then provide a numbered list of the commands. do not think out loud.\"\n",
    "                f\"Your response for case 1 should only include the command name\"\n",
    "                f\"Your response for case 2 should only include a reply and Here is a list of avaliable commands. Followed by the list of commands\"\n",
    "            )\n",
    "\n",
    "        # Create the messages for the model\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": self.system_role},\n",
    "            {\"role\": \"user\", \"content\": prompt},\n",
    "        ]\n",
    "\n",
    "        try:\n",
    "            # Generate the response\n",
    "            completion = self.client.chat.completions.create(\n",
    "                model=\"Qwen/Qwen2.5-Coder-32B-Instruct\",  # Replace with the correct model\n",
    "                messages=messages,\n",
    "                max_tokens=100,  # Adjust the token limit as needed\n",
    "            )\n",
    "\n",
    "            # Get and return the model's response\n",
    "            return completion.choices[0].message[\"content\"].strip()\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error occurred: {e}\")\n",
    "            return \"An error occurred. Please try again.\"\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "personal_assistant = PersonalAssistant(client)\n",
    "personal_assistant.interact_with_model()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
