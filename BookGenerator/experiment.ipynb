{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import requests\n",
    "from huggingface_hub import InferenceClient\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Get token from .env file\n",
    "hf_token = os.getenv('HUGGING_FACE_TOKEN')\n",
    "\n",
    "if not hf_token:\n",
    "    raise ValueError(\"HUGGING_FACE_TOKEN not found in .env file\")\n",
    "\n",
    "print(\"Token loaded successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "client = InferenceClient(api_key=hf_token)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StoryGenerator:\n",
    "    def __init__(self, client,role=\"You are a story writer\",max_tokens=3000,editor=None \n",
    "                 ,story_size=\"small\", model=\"Qwen/Qwen2.5-Coder-32B-Instruct\"):\n",
    "        self.client = client\n",
    "        self.toc = None\n",
    "        self.chapter_titles = None\n",
    "        self.story = None\n",
    "        self.story_size = story_size\n",
    "        self.model = model\n",
    "        self.story_details = None\n",
    "        self.num_chapters = 5\n",
    "        self.system_role = role\n",
    "        self.regenerate_chapters = False\n",
    "        #add dynamic token handeling\n",
    "        self.max_tokens =max_tokens\n",
    "\n",
    "        #ToDo: Implement BookEditor first\n",
    "        self.editor = editor\n",
    "\n",
    "    def write_story(self):\n",
    "      with open('generated_story.txt', 'w') as file:\n",
    "          file.write(self.story)\n",
    "\n",
    "      print(\"\\nStory saved to 'generated_story.txt'\")\n",
    "\n",
    "    def determine_book_size(self,user_input=\"small\"):\n",
    "        \n",
    "        if user_input == \"small\":\n",
    "            self.story_size = \"small\"\n",
    "        elif user_input == \"medium\":\n",
    "            self.story_size = \"medium\"\n",
    "        elif user_input == \"large\":\n",
    "            self.story_size = \"large\"\n",
    "        else:\n",
    "            print(\"Invalid input. Defaulting to small.\")\n",
    "            self.story_size = \"small\"\n",
    "\n",
    "    \n",
    "    def build_all_chapters(self, chapter_titles):\n",
    "      # Generate the content for each chapter\n",
    "      self.story = self.toc + \"\\n\\n\"\n",
    "      previous_summary = \"Introduction to the story.\"  # Initial summary for the Prologue\n",
    "\n",
    "      for i, chapter_name in enumerate(chapter_titles):\n",
    "          previous_chapter = chapter_titles[i - 1] if i > 0 else \"N/A\"\n",
    "          next_chapter = chapter_titles[i + 1] if i < len(chapter_titles) - 1 else \"N/A\"\n",
    "\n",
    "          print(f\"\\nGenerating content for {chapter_name}...\")\n",
    "          until_end = len(chapter_titles) - i -1\n",
    "          # Generate the content for the current chapter\n",
    "          chapter_content, chapter_summary = self.generate_chapter(model=self.model,\n",
    "             client=self.client, chapter_name=chapter_name, story_details=self.story_details, \n",
    "             previous_chapter=previous_chapter, next_chapter=next_chapter, previous_summary=previous_summary, \n",
    "             until_end= until_end\n",
    "          )\n",
    "\n",
    "\n",
    "          # Append the chapter content and summary to the story\n",
    "          self.story += f\"Chapter {i + 1}: {chapter_name}\\n\\n{chapter_content}\\n\\n\"\n",
    "\n",
    "          # Update the previous summary for the next iteration\n",
    "          previous_summary = chapter_summary\n",
    "\n",
    "      \n",
    "    def get_completions(self,messages, client, model=\"Qwen/Qwen2.5-Coder-32B-Instruct\",buffer=100, max_tokens=300):\n",
    "      new_message = {\"role\": \"assistant\", \"content\": \"Make sure your responses are complete and there are no cutoffs.\"}\n",
    "      messages.append(new_message)\n",
    "      completion = client.chat.completions.create(\n",
    "                        model=self.model,\n",
    "                        messages=messages,\n",
    "                        max_tokens=max_tokens+buffer\n",
    "                    )\n",
    "      return completion\n",
    "\n",
    "    def determine_dynamic_story_size(self, story_details):\n",
    "        word_count = len(story_details.split())\n",
    "\n",
    "        if word_count < 50:\n",
    "            return \"small\"\n",
    "        elif word_count < 200:\n",
    "            return \"medium\"\n",
    "        else:\n",
    "            return \"large\"\n",
    "\n",
    "    def build_story(self):\n",
    "      user_input = input(\"Would you like the model to build a story for you.(yes/no)\\n\").strip()\n",
    "      regen_input = input(\"Would you like to assist the model in improving chapters through chapter regeneration.(yes/no)\\n\").strip()\n",
    "      if regen_input.lower() == \"yes\":\n",
    "        print(\"Chapter regeneration is enabled.\")\n",
    "        self.regenerate_chapters = True\n",
    "      else:\n",
    "        print(\"Chapter regeneration is disabled\")\n",
    "        self.regenerate_chapters = False\n",
    "\n",
    "      story_sizes = [\"small\",\"medium\",\"large\"]\n",
    "\n",
    "      if user_input.lower() == \"yes\":\n",
    "        \n",
    "\n",
    "        # messages = [\n",
    "        #     {\"role\": \"system\", \"content\": self.system_role},\n",
    "        #     {\"role\": \"user\", \"content\": \"How large is your story reply only with small, medium, large\"},\n",
    "        # ]\n",
    "        # model_input = self.get_completions(messages, self.client, model=\"Qwen/Qwen2.5-Coder-32B-Instruct\",buffer=100, max_tokens=100)\n",
    "        # model_input = model_input.choices[0].message[\"content\"].strip()\n",
    "        # print(f'Story size set to: {model_input}')\n",
    "        # self.determine_book_size(user_input=model_input)\n",
    "\n",
    "        #Mabey should make story size use a randomizer instead of calling model\n",
    "        idx = random.randint(0, len(story_sizes) - 1)\n",
    "        story_size = story_sizes[idx]\n",
    "        print(f'Story size set to: {story_size}')\n",
    "        self.determine_book_size(user_input=story_size)\n",
    "\n",
    "        if self.story_size == \"small\":\n",
    "            max_tokens = 200\n",
    "            buffer = 500\n",
    "        elif self.story_size == \"medium\":\n",
    "            max_tokens = 300\n",
    "            buffer = 600\n",
    "        elif self.story_size == \"large\":\n",
    "            max_tokens = 400\n",
    "            buffer = 700\n",
    "        else:\n",
    "          max_tokens = 212\n",
    "          buffer = 500\n",
    "\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": self.system_role},\n",
    "            {\"role\": \"user\", \"content\": f\"Describe your {self.story_size} sized story within {max_tokens} tokens. Be sure to highlight core story details and make sure your response only includes story details:\"},\n",
    "        ]\n",
    "        model_input = self.get_completions(messages, self.client, model=self.model,buffer=buffer, max_tokens=max_tokens)\n",
    "        model_input = model_input.choices[0].message[\"content\"].strip()\n",
    "        print(f'Generated Story Description: {model_input}')\n",
    "        self.parse_story_details(self.client,user_input=model_input, model=self.model)\n",
    "        \n",
    "\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": self.system_role},\n",
    "            {\"role\": \"user\", \"content\": f\"How many chapters should the {self.story_size} sized story have with story details {model_input}? either choose random or choose a number between {2} and {10}: Make sure your response only includes random or a small number \"},\n",
    "        ]\n",
    "        model_input = self.get_completions(messages, self.client, model=self.model,buffer=5, max_tokens=30)\n",
    "        model_input = model_input.choices[0].message[\"content\"].strip()\n",
    "        \n",
    "        self.determine_amount_of_chapters(chapter_input=model_input)\n",
    "      else:\n",
    "        # Initial story size determination\n",
    "        user_input = input(\"How large is your story: small, medium, large?\\n\").strip()\n",
    "        self.determine_book_size(user_input=user_input)\n",
    "\n",
    "        # Story details input\n",
    "        story_details = input(\"Describe your story. Be sure to highlight core story details:\\n\").strip()\n",
    "\n",
    "        # Dynamically re-determine the size\n",
    "        # new_size = self.determine_dynamic_story_size(story_details)\n",
    "        # current_size = user_input\n",
    "\n",
    "        # if new_size != current_size:\n",
    "        #     print(f\"The story size appears to be better suited as '{new_size}'. Do you accept this change? (yes/no)\")\n",
    "        #     change_size = input().strip().lower()\n",
    "\n",
    "        #     if change_size == \"yes\":\n",
    "        #         print(f\"Story size has been updated to '{new_size}'.\")\n",
    "        #         self.determine_book_size(user_input=new_size)\n",
    "        #     else:\n",
    "        #         print(f\"Keeping the story size as '{current_size}'.\")\n",
    "\n",
    "        # Proceed to parse story details\n",
    "        self.parse_story_details(self.client, user_input=story_details, model=self.model)\n",
    "        \n",
    "        user_input = input(\"How many chapters should the story have? (Enter a number or type 'random'): \").strip()\n",
    "        self.determine_amount_of_chapters(chapter_input=user_input)\n",
    "\n",
    "      \n",
    "\n",
    "      self.generate_toc(self.num_chapters)\n",
    "\n",
    "      self.build_all_chapters(chapter_titles=self.chapter_titles)\n",
    "\n",
    "      # print(self.story)\n",
    "      self.write_story()\n",
    "      return self.story\n",
    "\n",
    "    def repair_truncated_json(self, raw_response):\n",
    "      \"\"\"\n",
    "      Attempts to repair truncated JSON by appending missing brackets/braces.\n",
    "      \"\"\"\n",
    "      try:\n",
    "          return json.loads(raw_response)  # First try parsing as-is\n",
    "      except json.JSONDecodeError:\n",
    "          # If parsing fails, attempt to repair\n",
    "          repaired_response = raw_response.rstrip(\",}\")  # Remove trailing commas or brackets\n",
    "          stack = []  # Track opening braces and brackets\n",
    "          for char in repaired_response:\n",
    "              if char in \"{[\":\n",
    "                  stack.append(char)\n",
    "              elif char in \"}]\":\n",
    "                  if stack and ((char == \"}\" and stack[-1] == \"{\") or (char == \"]\" and stack[-1] == \"[\")):\n",
    "                      stack.pop()\n",
    "\n",
    "          # Close remaining unbalanced braces/brackets\n",
    "          while stack:\n",
    "              last = stack.pop()\n",
    "              repaired_response += \"}\" if last == \"{\" else \"]\"\n",
    "\n",
    "          try:\n",
    "              return json.loads(repaired_response)  # Try parsing repaired response\n",
    "          except json.JSONDecodeError:\n",
    "              print(\"Error: Unable to repair the JSON.\")\n",
    "              return None\n",
    "\n",
    "    def update_story_details(self,details, story_summary):\n",
    "        print(\"Updating story details\")\n",
    "        messages = [\n",
    "                {\"role\": \"system\", \"content\": \"You are a story writer that needs to update the current story details structured in valid JSON format.\"},\n",
    "                {\"role\": \"user\", \"content\": f\"Current details: {details}\\n\\nSummary of chapter:{story_summary}\\n\\nTry not to exceed {1000} tokens \\n\\nRespond with updated details in valid JSON format. Do not change current story details unless needed. Make sure you update/include details that are important to the story.\"}\n",
    "            ]\n",
    "        try:\n",
    "            # completion = client.chat.completions.create(\n",
    "            #     model=model,\n",
    "            #     messages=messages,\n",
    "            #     max_tokens=max_tokens+100\n",
    "            # )\n",
    "            completion = self.get_completions(messages=messages, model=self.model,client=self.client,max_tokens=2000,buffer=500)\n",
    "            raw_response = completion.choices[0].message[\"content\"].strip()\n",
    "            self.story_details = self.repair_truncated_json(raw_response)\n",
    "            if not self.story_details:\n",
    "                print(\"Error: Could not parse or repair the JSON. Raw response:\")\n",
    "                print(raw_response)\n",
    "                return None\n",
    "\n",
    "            print(\"\\nNew story Details:\")\n",
    "            print(json.dumps(self.story_details, indent=4))\n",
    "            return self.story_details\n",
    "        except Exception as e:\n",
    "            print(f\"Error during model interaction: {e}\")\n",
    "            return None\n",
    "\n",
    "    def parse_story_details(self, client, details = None, model=\"Qwen/Qwen2.5-Coder-32B-Instruct\",user_input=\"Suprise me with a story\"):\n",
    "        \"\"\"\n",
    "        Parses story details and ensures the output is complete and valid JSON.\n",
    "        \"\"\"\n",
    "        if details == None:\n",
    "          story_prompt = (\n",
    "              \"Describe your story. Include details about the plot, setting, timeline, main characters, \"\n",
    "              \"and any other key elements. If some details are missing, the system will infer them for you.\"\n",
    "          )\n",
    "          \n",
    "          \n",
    "          if self.story_size == \"small\":\n",
    "            max_tokens = 312\n",
    "            buffer = 300\n",
    "          elif self.story_size == \"medium\":\n",
    "            max_tokens = 412\n",
    "            buffer = 400\n",
    "          elif self.story_size == \"large\":\n",
    "            max_tokens = 512\n",
    "            buffer = 500\n",
    "          else:\n",
    "            max_tokens = 312\n",
    "            buffer =300\n",
    "\n",
    "          messages = [\n",
    "              {\"role\": \"system\", \"content\": \"You are an assistant that creates structured story details in valid JSON format.\"},\n",
    "              {\"role\": \"user\", \"content\": f\"{story_prompt}\\n\\n{user_input}\\n\\nRespond with details in valid JSON format. Make sure that the response is less than {max_tokens} tokens.\"}\n",
    "          ]\n",
    "        else:\n",
    "          if self.story_size == \"small\":\n",
    "            max_tokens = 400\n",
    "            buffer = 300\n",
    "          elif self.story_size == \"medium\":\n",
    "            max_tokens = 500\n",
    "            buffer = 400\n",
    "          elif self.story_size == \"large\":\n",
    "            max_tokens = 600\n",
    "            buffer = 500\n",
    "          else:\n",
    "            max_tokens = 212\n",
    "            buffer = 200\n",
    "          messages = [\n",
    "              {\"role\": \"system\", \"content\": \"You are an assistant that creates structured details in valid and complete JSON format.\"},\n",
    "              {\"role\": \"user\", \"content\": f\"Here are the details of the current story in json: {self.story_details}.Update the current story details with this factor to consider: {details}\\n\\nRespond with details in valid JSON format making sure to include a plot that summarizes all important details. Make sure that the response is less than {max_tokens} tokens.\"}\n",
    "          ]\n",
    "\n",
    "        try:\n",
    "            if details == None:\n",
    "              completion = client.chat.completions.create(\n",
    "                  model=model,\n",
    "                  messages=messages,\n",
    "                  max_tokens=max_tokens+buffer\n",
    "              )\n",
    "            else:\n",
    "              completion = client.chat.completions.create(\n",
    "                  model=model,\n",
    "                  messages=messages,\n",
    "                  max_tokens=max_tokens+1000\n",
    "              )\n",
    "            # completion = self.get_completions(messages=messages, model=model,client=client,max_tokens=max_tokens,buffer=buffer)\n",
    "            raw_response = completion.choices[0].message[\"content\"].strip()\n",
    "            self.story_details = self.repair_truncated_json(raw_response)\n",
    "            if not self.story_details:\n",
    "                print(\"Error: Could not parse or repair the JSON. Raw response:\")\n",
    "                print(raw_response)\n",
    "                return None\n",
    "\n",
    "            print(\"\\nStory Details:\")\n",
    "            print(json.dumps(self.story_details, indent=4))\n",
    "            return self.story_details\n",
    "        except Exception as e:\n",
    "            print(f\"Error during model interaction: {e}\")\n",
    "            return None\n",
    "\n",
    "    # Example usage ( need to replace `client` with your actual model client)\n",
    "    # story_details = parse_story_details(client)\n",
    "\n",
    "    def generate_chapter(\n",
    "        self,\n",
    "        client,\n",
    "        chapter_name,\n",
    "        story_details,\n",
    "        previous_chapter,\n",
    "        next_chapter,\n",
    "        previous_summary,\n",
    "        until_end,\n",
    "        model=\"Qwen/Qwen2.5-Coder-32B-Instruct\",\n",
    "        chapter_details=\"N/a\",  # Default to \"N/a\" if no specific chapter details are provided\n",
    "        completion_criteria=\"Ensure this chapter has a clear beginning, middle, and end, with no loose ends.\"\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Generate the content of a chapter given its context and ensure it is complete.\n",
    "        The user will be able to view the chapter summary and decide whether to regenerate it or not.\n",
    "        They can also specify any changes they'd like to apply to the story details.\n",
    "        \"\"\"\n",
    "        if self.story_size == \"small\":\n",
    "            max_tokens = 600\n",
    "            buffer = 400\n",
    "        elif self.story_size == \"medium\":\n",
    "            max_tokens = 800\n",
    "            buffer =600\n",
    "        elif self.story_size == \"large\":\n",
    "            max_tokens = 1000\n",
    "            buffer = 800\n",
    "        else:\n",
    "          max_tokens = 412\n",
    "          buffer = 200\n",
    "\n",
    "        # Create the chapter generation prompt\n",
    "        chapter_prompt = (\n",
    "            f\"Story details: {story_details}\\n\"\n",
    "            f\"Chapter details: {chapter_details}\\n\"\n",
    "            f\"Current chapter title: '{chapter_name}'.\\n\"\n",
    "            f\"Previous chapter title: '{previous_chapter}'.\\n\"\n",
    "            f\"Next chapter title: '{next_chapter}'.\\n\"\n",
    "            f\"Summary of the previous chapter: {previous_summary}\\n\"\n",
    "            f\"Chapters until the last chapter: {until_end}\\n\"\n",
    "            f\"{completion_criteria}\\n\"\n",
    "            f\"Write the content for this chapter. Be sure to include nothing but the content and not the title info. \"\n",
    "            f\"Finish the chapter using no more than {max_tokens} tokens.\"\n",
    "        )\n",
    "\n",
    "        # Create the messages for the model\n",
    "        messages = [\n",
    "              {\"role\": \"system\", \"content\": \"You are an assistant that creates story chapters using all the provided information.\"},\n",
    "              {\"role\": \"user\", \"content\": f\"{chapter_prompt}\"}\n",
    "          ]\n",
    "\n",
    "        #make function get_completion\n",
    "        try:\n",
    "            # completion = client.chat.completions.create(\n",
    "            #     model=model,\n",
    "            #     messages=messages,\n",
    "            #     max_tokens=max_tokens+200\n",
    "            # )\n",
    "            completion = self.get_completions(messages=messages, model=model,client=client,max_tokens=max_tokens,buffer=buffer)\n",
    "        except Exception as e:\n",
    "            print(f\"Error during completion: {e}\")\n",
    "            return None, None\n",
    "        \n",
    "        # Get the generated chapter content\n",
    "        chapter_content = completion.choices[0].message[\"content\"].strip()\n",
    "\n",
    "        # Summarize the chapter\n",
    "        chapter_summary = self.summarize_chapter(chapter_content)\n",
    "\n",
    "        # Present the summary and ask the user if they want to regenerate the chapter\n",
    "        print(f\"\\nChapter {chapter_name} Summary:\")\n",
    "        print(chapter_summary)\n",
    "\n",
    "        # regenerate_chapter(\n",
    "        #     chapter_name,\n",
    "        #     story_details,\n",
    "        #     previous_chapter,\n",
    "        #     next_chapter,\n",
    "        #     previous_summary,\n",
    "        #     until_end,\n",
    "        #     chapter_details=chapter_details,\n",
    "        #     completion_criteria=completion_criteria\n",
    "        # )\n",
    "\n",
    "        # Ask the user if they are satisfied with the chapter or if they want to regenerate\n",
    "        if self.regenerate_chapters == True:\n",
    "          regenerate_decision = input(\"\\nDo you want to regenerate this chapter? (yes/no): \").strip().lower()\n",
    "\n",
    "          if regenerate_decision == \"yes\":\n",
    "              # Ask the user if they want to modify story details\n",
    "              modify_decision = input(\"Would you like to modify the story details? (yes/no): \").strip().lower()\n",
    "\n",
    "              additional_story_details = {}\n",
    "\n",
    "              if modify_decision == \"yes\":\n",
    "                  # Get additional details from the user to update story_details\n",
    "                  additional_details = input(\"Please provide additional story details (or type 'none' if no modifications): \").strip()\n",
    "\n",
    "                  if additional_details.lower() != 'none':\n",
    "                      # Parse the new details using the same function\n",
    "                      self.parse_story_details(client=client, details=additional_details)\n",
    "\n",
    "                      # Ensure we merge the new details with the current story details\n",
    "                      # if additional_story_details:\n",
    "                      #     self.story_details.update(additional_story_details)\n",
    "                      #     print(f\"Story details updated with new information:\\n {story_details}\")\n",
    "                      # else:\n",
    "                      #     print(\"Error: Failed to parse new details. Proceeding with the existing story details.\")\n",
    "\n",
    "              # Extract plot from updated story details to pass as chapter_details\n",
    "              new_chapter_details = self.story_details.get(\"plot\", \"N/a\") if additional_story_details else chapter_details\n",
    "\n",
    "              print(f\"Regenerating chapter...plot:{new_chapter_details}\")\n",
    "              return self.generate_chapter(\n",
    "                  client=self.client,\n",
    "                  chapter_name=chapter_name,\n",
    "                  #make  self.story_details if the the story_details are not too much\n",
    "                  story_details=self.story_details,\n",
    "                  previous_chapter=previous_chapter,\n",
    "                  next_chapter=next_chapter,\n",
    "                  previous_summary=previous_summary,\n",
    "                  until_end=until_end,\n",
    "                  completion_criteria=completion_criteria,\n",
    "                  chapter_details=new_chapter_details,\n",
    "                  model=model\n",
    "                  \n",
    "              )\n",
    "        #may need to remove\n",
    "        #self.update_story_details(self.story_details, chapter_summary)\n",
    "        return chapter_content, chapter_summary\n",
    "\n",
    "\n",
    "\n",
    "    def determine_amount_of_chapters(self,chapter_input=5):\n",
    "      # Ask the user for chapter details\n",
    "      if chapter_input.lower() == \"random\":\n",
    "          self.num_chapters = random.randint(3, 15)  # Random number between 5 and 15\n",
    "      else:\n",
    "          try:\n",
    "              self.num_chapters = int(chapter_input)\n",
    "              print(f\"Number of chapters in story: {self.num_chapters}\")\n",
    "          except ValueError:\n",
    "              print(\"Invalid input. Defaulting to 5 chapters.\")\n",
    "              self.num_chapters = 5\n",
    "\n",
    "    \n",
    "    def generate_toc(self, num_chapters, model=\"Qwen/Qwen2.5-Coder-32B-Instruct\"):\n",
    "      self.toc = \"Table of Contents\\n\\n\"\n",
    "      prev_title = \"Prologue\"  # Start with the Prologue\n",
    "      self.toc += f\"Chapter 1: {prev_title}\\n\"\n",
    "      self.chapter_titles = [prev_title]  # Initialize with the first chapter's title\n",
    "\n",
    "      for i in range(2, num_chapters + 1):\n",
    "          # Join previous titles into a single string to include in the prompt\n",
    "          previous_titles_str = ', '.join(self.chapter_titles)\n",
    "\n",
    "          # Create the chapter prompt ensuring uniqueness\n",
    "          chapter_prompt = (\n",
    "              f\"Here are the intitial details of the story {self.story_details} Previous chapter titles: '{previous_titles_str}'. \"\n",
    "              f\"Suggest only the concise title for Chapter {i} based off the previous title {prev_title}. The title must be unique and not repeat any previous titles. \"\n",
    "              f\"Make sure that your response is nothing other than than title of the chapter\"\n",
    "              f\"The end of the story will be in {num_chapters-i} chapters make sure to consider this.\"\n",
    "          )\n",
    "\n",
    "\n",
    "          # Create the messages for the model\n",
    "          messages = [\n",
    "            {\"role\": \"system\", \n",
    "            \"content\": \"You are a creative writer that creates good chapter titles.\"},\n",
    "            {\n",
    "              \"role\": \"user\",\n",
    "              \"content\": f\"{chapter_prompt}\"\n",
    "            }\n",
    "          ]\n",
    "\n",
    "          # Generate the title\n",
    "          completion = client.chat.completions.create(\n",
    "              model=self.model,\n",
    "              messages=messages,\n",
    "              max_tokens=15\n",
    "          )\n",
    "\n",
    "          next_title = completion.choices[0].message[\"content\"].strip()\n",
    "          # Ensure the title is unique by checking it against all previous titles\n",
    "\n",
    "          # while next_title in self.chapter_titles:\n",
    "          #     print(f\"Duplicate title detected: {next_title}. Generating a new title...\")\n",
    "\n",
    "          #     # Regenerate the title if it's a duplicate\n",
    "          #     completion = client.chat.completions.create(\n",
    "          #         model=model,\n",
    "          #         messages=messages,\n",
    "          #         max_tokens=15\n",
    "          #     )\n",
    "          #     next_title = completion.choices[0].message[\"content\"].strip()\n",
    "\n",
    "          # Append the new title and update the table of contents\n",
    "          self.chapter_titles.append(next_title)\n",
    "          self.toc += f\"Chapter {i}: {next_title}\\n\"\n",
    "\n",
    "          # Update the previous title for the next iteration\n",
    "          prev_title = next_title  # Update for the next iteration\n",
    "\n",
    "    # Function to summarize a chapter\n",
    "    def summarize_chapter(self,chapter_content, model=\"Qwen/Qwen2.5-Coder-32B-Instruct\"):\n",
    "        \"\"\"Generate a brief summary of a chapter.\"\"\"\n",
    "        if self.story_size == \"small\":\n",
    "            max_tokens = 512\n",
    "            buffer = 300\n",
    "        elif self.story_size == \"medium\":\n",
    "            max_tokens = 612\n",
    "            buffer = 400\n",
    "        elif self.story_size == \"large\":\n",
    "            max_tokens = 712\n",
    "            buffer = 500\n",
    "        else:\n",
    "          max_tokens = 312\n",
    "          buffer = 300\n",
    "        \n",
    "        summary_prompt = f\"Summarize this chapter with the most important story facts in detailed sentences under {max_tokens} tokens: Be sure to mention all of the important story building details\\n{chapter_content}\"\n",
    "\n",
    "\n",
    "        # Create the messages for the model\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": \"You are a creative writer that is trying to summarize chapters.\"},\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": f\"{summary_prompt}\"\n",
    "            }\n",
    "        ]\n",
    "        \n",
    "        # Use the client to call the model and generate the summary\n",
    "        # completion = self.client.chat.completions.create(\n",
    "        #     model=model,  # Use the same model as before\n",
    "        #     messages=messages,\n",
    "        #     max_tokens=max_tokens +100  # You can adjust max_tokens as needed for the summary length\n",
    "        # )\n",
    "        completion = self.get_completions(messages=messages, model=model,client=client,max_tokens=max_tokens,buffer=buffer)\n",
    "\n",
    "        # Get the generated summary\n",
    "        summary = completion.choices[0].message[\"content\"].strip()\n",
    "\n",
    "        return summary\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen = StoryGenerator(client)\n",
    "gen.build_story()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
